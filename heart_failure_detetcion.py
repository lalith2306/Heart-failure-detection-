# -*- coding: utf-8 -*-
"""Heart Failure Detetcion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yfz-8q_mlbGDpvbx01y1QIoHQWS7BNT5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection  import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import mean_squared_error , accuracy_score,confusion_matrix, r2_score

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.svm import SVC

from sklearn.linear_model import LinearRegression

import seaborn as sns

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

from sklearn.naive_bayes import GaussianNB, CategoricalNB

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import classification_report

df=pd.read_csv('HeartAttack (1).csv',na_values='?')

df.head(5)

df.info()

df.describe()

df.isnull().sum()

sns.pairplot(df)

plt.figure(figsize=(20,20))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)

#feature extraction
df = df.drop(columns=['slope','thal','ca'])

df.head()

df = df.dropna()

df.head()

df.isnull().sum()

df['sex'].value_counts()

df['cp'].value_counts()

df['restecg'].value_counts()

"""converting categorical to one hot encoding



"""

df = pd.get_dummies(df,columns=['cp','restecg'])

df.head()

df.columns

df = df.rename(columns = {'num       ':'target'})

numerical_cols = ['age','trestbps','chol','thalach','oldpeak']
categorical_cols = list(set(df.columns)-set(numerical_cols)-{'target'})

df_train , df_test = train_test_split(df,test_size=0.2,random_state = 42)

len(df_train),len(df_test)

scaler = StandardScaler()

def get_features_and_target_arrays(df,numerical_cols,categorical_cols,scaler):
  x_numeric_scaled = scaler.fit_transform(df[numerical_cols])
  x_categorical = df[categorical_cols].to_numpy()
  x = np.hstack((x_categorical,x_numeric_scaled))
  y = df['target']
  return x,y

x_train , y_train = get_features_and_target_arrays(df_train,categorical_cols,numerical_cols,scaler)

"""### 1. Logistic Regression

"""

#train
clf = LogisticRegression()
clf.fit(x_train , y_train)

x_test , y_test = get_features_and_target_arrays(df_test,categorical_cols,numerical_cols,scaler)

test_pred = clf.predict(x_test)

mean_squared_error(y_test,test_pred)

accuracy_score(y_test,test_pred)

confusion_matrix(y_test,test_pred)

precision = precision_score(y_test, test_pred)
recall = recall_score(y_test, test_pred)
f1 = f1_score(y_test, test_pred)

print(precision)
print(recall)
print(f1)

"""### 2. Decision Tree

"""

dc_clf = DecisionTreeClassifier()
dc_clf.fit(x_train,y_train)
dc_pred = dc_clf.predict(x_test)
print(mean_squared_error(y_test,dc_pred))
print(accuracy_score(y_test,dc_pred))

precision = precision_score(y_test, dc_pred)
recall = recall_score(y_test, dc_pred)
f1 = f1_score(y_test, dc_pred)

print(precision)
print(recall)
print(f1)

"""### 3.RANDOM FOREST"""

rf_clf = RandomForestClassifier()
rf_clf.fit(x_train,y_train)
rf_pred = rf_clf.predict(x_test)
print(mean_squared_error(y_test,rf_pred))
print(accuracy_score(y_test,rf_pred))

precision = precision_score(y_test, rf_pred)
recall = recall_score(y_test, rf_pred)
f1 = f1_score(y_test, rf_pred)

print(precision)
print(recall)
print(f1)

"""### 4.Support Vector Machine

"""

svm_clf = SVC()
svm_clf.fit(x_train,y_train)
svm_pred = svm_clf.predict(x_test)
print(mean_squared_error(y_test,svm_pred))
print(accuracy_score(y_test,svm_pred))
auc = accuracy_score(y_test,svm_pred)

precision = precision_score(y_test, svm_pred)
recall = recall_score(y_test, svm_pred)
f1 = f1_score(y_test, svm_pred)

print(precision)
print(recall)
print(f1)



plt.plot(y_test, svm_pred)

"""### 5.K means clustering

"""

data_2d = df[["thalach", "chol"]]
data_2d.head()

#Create a scatter plot with 'Annual Income' on X-axis and 'Spending Score' on Y-axis.
plt.figure(figsize = (14,5))
plt.scatter(data_2d['thalach'], data_2d['chol'])
plt.title('thalach vs chol')
plt.xlabel('thalach')
plt.ylabel('chol')
plt.show()

#Determine the clusters for 2D dataset.
from sklearn.cluster import KMeans
kmeans_2d = KMeans(n_clusters = 5, random_state = 10)
kmeans_2d.fit(data_2d)
labels_2d = kmeans_2d.predict(data_2d)
print(labels_2d)

import warnings
warnings.filterwarnings("ignore")
data_2d['Label'] = labels_2d
data_2d.head()

#Plot the datapoints corresponding to clusters
plt.figure(figsize = (14, 5))
plt.scatter(data_2d['thalach'], data_2d['chol'], c = labels_2d)
plt.scatter(kmeans_2d.cluster_centers_[:, 0], kmeans_2d.cluster_centers_[:, 1], marker = 'x', c = 'black', s = 75)
plt.title('thalach vs chol')
plt.xlabel('thalach')
plt.ylabel('chol')
plt.show()

#Selecting k Using Elbow Method
wcss = []

clusters = range(1, 11)
# Initiate a for loop that ranges from 1 to 10.
for k in clusters :
    # Inside for loop, perform K-Means clustering for current value of K. Use 'fit()' to train the model.
    kmeans = KMeans(n_clusters = k, random_state = 10)
    kmeans.fit(data_2d)
    # Find wcss for current K value using 'inertia_' attribute and append it to the empty list.
    wcss.append(kmeans.inertia_)

# Create a DataFrame with two columns.
# First column must contain K values from 1 to 10 and second column must contain wcss values obtained after the for loop.
wcss_data = pd.DataFrame({'Clusters': clusters, 'WCSS': wcss})
wcss_data

#Plot WCSS vs number of clusters.
plt.figure(figsize=(14,5))
plt.title('WCSS (Within Cluster Sum of Squares)')
plt.plot(clusters, wcss)
plt.xlabel("K")
plt.ylabel("WCSS")
plt.grid()
plt.xticks(range(1,11))
plt.show()

"""### 6.LINEAR REGRESSION

"""

model = LinearRegression()
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error: {mse:.2f}')
print(f'R-squared: {r2:.2f}')

"""### 7.CNN

"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Standardize the features
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Build the neural network model
model = Sequential()
model.add(Dense(64, input_dim=x_train.shape[1], activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
predictions = model.predict(x_test)

mean_squared_error(y_test,predictions)

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Training Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""### 8.Adaboost"""

x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

# Create an AdaBoost model
adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)

# Train the model
adaboost_model.fit(x_train, y_train)

# Predict on the test set
y_pred = adaboost_model.predict(x_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print the results
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print('Confusion Matrix:')
print(conf_matrix)

"""### 9. Gaussian Naive Bayes"""

gnb = GaussianNB()
gnb.fit(x_train , y_train)

# Predict on the test set
y_pred = gnb.predict(x_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

# Print the results
print(f'Accuracy: {accuracy:.4f}')
print(f'Precision: {precision:.4f}')
print(f'Recall: {recall:.4f}')
print(f'F1 Score: {f1:.4f}')
print('Confusion Matrix:')
print(conf_matrix)

"""### 10.Gradient Boosting"""

gbc = GradientBoostingClassifier(random_state=42)

# Train the classifier
gbc.fit(x_train, y_train)

# Make predictions on the test set
y_pred = gbc.predict(x_test)

# Evaluate the classifier
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy:.2f}')
print('Confusion Matrix:')
print(conf_matrix)
print('Classification Report:')
print(classification_rep)

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(precision)
print(recall)
print(f1)

"""## Comparision of various algorithm"""

from sklearn import datasets

# Define classifiers
classifiers={ 'Logistic Regression ':LogisticRegression(random_state=42),
    'Decision Tree':DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'K-Means':KMeans(random_state=42),
    'Adaboost':AdaBoostClassifier(random_state=42),
    'Gaussian NB':GaussianNB(),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42)
}

# Train and evaluate each classifier
results = {}
for name, clf in classifiers.items():
    clf.fit(x_train, y_train)
    y_pred = clf.predict(x_test)
    accuracy = accuracy_score(y_test, y_pred)
    results[name] = accuracy
    print(f'{name} Accuracy: {accuracy:.2f}')

# Plot the results
names = list(results.keys())
values = list(results.values())

plt.barh(names, values)
plt.xlabel('Accuracy')
plt.title('Classifier Comparison')
plt.show()

